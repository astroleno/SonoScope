'use client';

import { useState, useRef } from 'react';
import Visualizer from '../components/visualizer';
import Meyda from 'meyda';
import { useDanmuPipeline } from '../hooks/useDanmuPipeline';
import MobileAudioPermission from '../components/mobile-audio-permission';
import AudioFallback from '../components/audio-fallback';
import {
  getSharedInstrumentClassifier,
  InstrumentDetectionResult,
} from '../lib/instrument-classifier';
import { EnhancedFeatureAggregator } from '../lib/enhanced-feature-aggregator';

type AudioFeatureSnapshot = {
  // åŸºç¡€Meydaç‰¹å¾
  rms?: number;
  spectralCentroid?: number;
  zcr?: number;
  mfcc?: number[];
  spectralFlatness?: number;
  spectralFlux?: number;
  chroma?: number[];
  spectralBandwidth?: number;
  spectralRolloff?: number;
  spectralContrast?: number[];
  spectralSpread?: number;
  spectralSkewness?: number;
  spectralKurtosis?: number;
  loudness?: number;
  perceptualSpread?: number;
  perceptualSharpness?: number;
  voiceProb?: number;
  percussiveRatio?: number;
  harmonicRatio?: number;
  dominantInstrument?: string;
  instrumentProbabilities?: Record<string, number>;
  instrumentConfidence?: number;
  
  // å¢å¼ºç‰¹å¾
  pitch?: {
    fundamentalFreq: number;
    pitchConfidence: number;
    pitchClass: string;
    octave: number;
    cents: number;
    harmonicity: number;
    isVoiced: boolean;
  };
  tempo?: {
    bpm: number;
    tempoConfidence: number;
    timeSignature: [number, number];
    rhythmPattern: string;
  };
  timbre?: {
    brightness: number;
    warmth: number;
    roughness: number;
    timbreCategory: string;
  };
  instruments?: {
    dominantInstrument: string;
    instrumentCount: number;
    polyphony: number;
  };
  enhancedHPSS?: {
    musicComplexity: number;
    overallStability: number;
    overallRichness: number;
    dominantComponent: 'harmonic' | 'percussive' | 'mixed';
  };
};

export default function HomePage() {
  const [isStarted, setIsStarted] = useState(false);
  const [status, setStatus] = useState('ç­‰å¾…å¯åŠ¨');
  const [audioLevel, setAudioLevel] = useState(0);
  const [error, setError] = useState<string | null>(null);
  const [deviceInfo, setDeviceInfo] = useState<string>('');
  const [testMode, setTestMode] = useState(false);
  const [showMobilePermission, setShowMobilePermission] = useState(false);
  const [showAudioFallback, setShowAudioFallback] = useState(false);
  const [signalOn, setSignalOn] = useState(false);
  const [peak, setPeak] = useState(0);
  const [preset, setPreset] = useState<
    'pulse' | 'accretion' | 'spiral' | 'mosaic'
  >('pulse');
  const [accretionCtrl, setAccretionCtrl] = useState({
    sensMin: 0.92,
    sensMax: 1.18,
    gainScale: 1.15,
    flickerStrength: 0.1,
    flickerFreq: 14,
    overallBoost: 1.1,
  });
  const [mosaicCtrl, setMosaicCtrl] = useState({
    cellSize: 20,
    maxAge: 80,
    growthRate: 0.05,
    spawnRate: 0.02,
    colorScheme: 0,
    colorFlowSpeed: 0.01,
    alpha: 0.7,
  });
  // const [frequencyBars, setFrequencyBars] = useState<number[]>([]); // legacy, not used now
  const [features, setFeatures] = useState<AudioFeatureSnapshot | null>(null);
  const [sensitivity, setSensitivity] = useState<number>(1.5);
  const [rawMic, setRawMic] = useState<boolean>(false);
  const [autoCalibrate, setAutoCalibrate] = useState<boolean>(true);
  const noiseFloorRef = useRef<number>(0);

  // å¼¹å¹•ç®¡çº¿
  const danmuPipeline = useDanmuPipeline({
    enabled: true,
    autoStart: false, // æ‰‹åŠ¨å¯åŠ¨
    useSimple: false, // å¯ç”¨å®Œæ•´ç‰ˆç®¡çº¿è¿›è¡Œå®æ—¶é£æ ¼æ£€æµ‹
    needComments: 4,
    locale: 'zh-CN',
    rmsThreshold: 0.0001, // è¿›ä¸€æ­¥é™ä½RMSé˜ˆå€¼ï¼Œç¡®ä¿èƒ½æ£€æµ‹åˆ°éŸ³é¢‘
    maxConcurrency: 2,
  });

  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const microphoneRef = useRef<MediaStreamAudioSourceNode | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const animationFrameRef = useRef<number | null>(null);
  const isRunningRef = useRef<boolean>(false);
  const meydaAnalyzerRef = useRef<any | null>(null);
  const featureSnapshotRef = useRef<AudioFeatureSnapshot | null>(null);
  const enhancedFeatureAggregatorRef = useRef<EnhancedFeatureAggregator | null>(null);
  type SharedInstrumentClassifier = ReturnType<
    typeof getSharedInstrumentClassifier
  >;
  const instrumentClassifierRef = useRef<SharedInstrumentClassifier | null>(
    null
  );
  const instrumentBufferRef = useRef<Float32Array>(new Float32Array(0));
  const instrumentSampleRateRef = useRef<number>(44100);
  const instrumentLastEvalRef = useRef<number>(0);
  const instrumentPendingRef = useRef<boolean>(false);
  const instrumentInfoRef = useRef<InstrumentDetectionResult | null>(null);
  const voiceLevelRef = useRef(0);
  const percussiveLevelRef = useRef(0);
  const harmonicLevelRef = useRef(0.5);

  const clamp01 = (v: number) => Math.max(0, Math.min(1, v));
  const clampOptional = (value: number | undefined) => {
    if (typeof value !== 'number' || Number.isNaN(value) || !Number.isFinite(value)) {
      return 0;
    }
    return clamp01(value);
  };
  const pickNumber = (value: unknown): number | undefined => {
    if (typeof value !== 'number') return undefined;
    if (Number.isNaN(value) || !Number.isFinite(value)) return undefined;
    return value;
  };
  const toFixedArray = (value: unknown, length: number) => {
    if (!Array.isArray(value)) return undefined;
    const result = new Array(length).fill(0);
    for (let i = 0; i < Math.min(length, value.length); i++) {
      const num = pickNumber(value[i]);
      if (num !== undefined) {
        result[i] = num;
      }
    }
    return result;
  };

  const deriveAudioHints = (f: any) => {
    const flat =
      typeof f?.spectralFlatness === 'number' ? f.spectralFlatness : undefined;
    const centroid =
      typeof f?.spectralCentroid === 'number' ? f.spectralCentroid : undefined;
    const flux =
      typeof f?.spectralFlux === 'number' ? f.spectralFlux : undefined;

    let voiceProb: number | undefined;
    if (flat != null || centroid != null) {
      const flatFactor = flat != null ? clamp01(1 - flat) : 0.5;
      const centroidNorm =
        centroid != null ? clamp01((centroid - 1500) / 2500) : 0.5;
      voiceProb = clamp01(0.35 + 0.4 * flatFactor + 0.25 * centroidNorm);
    }

    let percussiveRatio: number | undefined;
    if (flat != null || flux != null) {
      const fluxNorm = flux != null ? clamp01(flux * 1.4) : 0.35;
      const flatNorm = flat != null ? clamp01(flat) : 0.4;
      percussiveRatio = clamp01(0.45 * fluxNorm + 0.4 * flatNorm);
    }

    let harmonicRatio: number | undefined;
    if (percussiveRatio != null) {
      const base = 1 - percussiveRatio * 0.7;
      const voiceBoost = voiceProb != null ? 0.2 * voiceProb : 0;
      harmonicRatio = clamp01(base + voiceBoost);
    }

    return { voiceProb, percussiveRatio, harmonicRatio };
  };

  const appendInstrumentSamples = (chunk: Float32Array) => {
    const sampleRate =
      instrumentSampleRateRef.current ||
      audioContextRef.current?.sampleRate ||
      44100;
    const maxSamples = Math.max(sampleRate * 2, chunk.length);
    const current = instrumentBufferRef.current;
    const combinedLength = Math.min(maxSamples, current.length + chunk.length);
    const nextBuffer = new Float32Array(combinedLength);
    const tailLength = Math.min(current.length, combinedLength - chunk.length);
    if (tailLength > 0) {
      nextBuffer.set(current.slice(current.length - tailLength), 0);
    }
    nextBuffer.set(chunk, combinedLength - chunk.length);
    instrumentBufferRef.current = nextBuffer;
  };

  const runInstrumentDetection = async () => {
    if (instrumentPendingRef.current) return;
    const sampleRate =
      instrumentSampleRateRef.current ||
      audioContextRef.current?.sampleRate ||
      44100;
    const buffer = instrumentBufferRef.current;
    if (buffer.length < sampleRate) return;
    const now =
      typeof performance !== 'undefined' ? performance.now() : Date.now();
    if (now - instrumentLastEvalRef.current < 800) return;

    instrumentPendingRef.current = true;
    instrumentLastEvalRef.current = now;

    try {
      if (!instrumentClassifierRef.current) {
        instrumentClassifierRef.current = getSharedInstrumentClassifier();
      }
      const segment = buffer.slice(buffer.length - sampleRate);
      const result = await instrumentClassifierRef.current.classify(
        segment,
        sampleRate
      );
      if (result) {
        instrumentInfoRef.current = result;
      }
    } catch (err) {
      console.warn('Instrument detection error:', err);
    } finally {
      instrumentPendingRef.current = false;
    }
  };

  const handleStart = () => {
    // æ£€æµ‹ç§»åŠ¨è®¾å¤‡å¹¶æ˜¾ç¤ºç›¸åº”çš„æƒé™è¯·æ±‚
    const isMobile = /Mobile|Android|iPhone|iPad|iPod/i.test(
      navigator.userAgent
    );

    if (isMobile) {
      setShowMobilePermission(true);
    } else {
      // æ¡Œé¢è®¾å¤‡ä½¿ç”¨åŸæ¥çš„æµç¨‹
      startDesktopAudio();
    }
  };

  const startDesktopAudio = async () => {
    try {
      setStatus('è¯·æ±‚éº¦å…‹é£æƒé™...');
      console.log('è¯·æ±‚éº¦å…‹é£æƒé™...');

      // å…ˆè¯·æ±‚æƒé™ï¼Œå† enumerateDevicesï¼Œä»¥é¿å… Safari/æƒé™ç­–ç•¥å¯¼è‡´çš„è®¾å¤‡åˆ—è¡¨ç©º/æ— æ ‡ç­¾
      const provisionalStream = await navigator.mediaDevices.getUserMedia({
        audio: true,
      });

      setStatus('æ£€æŸ¥éŸ³é¢‘è®¾å¤‡...');
      console.log('æ£€æŸ¥éŸ³é¢‘è®¾å¤‡...');
      const devices = await navigator.mediaDevices.enumerateDevices();
      const audioInputs = devices.filter(d => d.kind === 'audioinput');
      if (audioInputs.length === 0) {
        // æ¸…ç†ä¸´æ—¶æµ
        provisionalStream.getTracks().forEach(t => t.stop());
        throw new Error('æœªæ£€æµ‹åˆ°éŸ³é¢‘è¾“å…¥è®¾å¤‡ï¼Œè¯·è¿æ¥éº¦å…‹é£æˆ–è€³æœº');
      }

      // ä¼˜å…ˆä½¿ç”¨æœ‰æ ‡ç­¾çš„ç¬¬ä¸€ä¸ªè®¾å¤‡
      const preferred = audioInputs.find(d => !!d.label) ?? audioInputs[0];
      const deviceId = preferred.deviceId;

      // å…³é—­ä¸´æ—¶æµï¼Œä½¿ç”¨æŒ‡å®š deviceId é‡æ–°è·å–åª’ä½“æµ
      provisionalStream.getTracks().forEach(t => t.stop());

      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          deviceId: deviceId ? { exact: deviceId } : undefined,
          echoCancellation: rawMic ? false : true,
          noiseSuppression: rawMic ? false : true,
          autoGainControl: rawMic ? false : true,
          sampleRate: 44100,
        },
      });

      // æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
      const nameList = audioInputs.map(d => d.label || 'æœªçŸ¥è®¾å¤‡').join(', ');
      setDeviceInfo(
        `æ£€æµ‹åˆ° ${audioInputs.length} ä¸ªéŸ³é¢‘è®¾å¤‡ï¼Œå½“å‰: ${preferred.label || 'æœªçŸ¥'}ï¼›å…¨éƒ¨: ${nameList}`
      );

      setStatus('åˆå§‹åŒ–éŸ³é¢‘åˆ†æ...');
      console.log('åˆå§‹åŒ–éŸ³é¢‘åˆ†æ...');

      // åˆ›å»ºéŸ³é¢‘ä¸Šä¸‹æ–‡
      const audioContext = new (window.AudioContext ||
        (window as any).webkitAudioContext)();
      const analyser = audioContext.createAnalyser();
      const microphone = audioContext.createMediaStreamSource(stream);
      instrumentSampleRateRef.current = audioContext.sampleRate;
      instrumentBufferRef.current = new Float32Array(0);
      instrumentLastEvalRef.current = 0;
      instrumentInfoRef.current = null;
      instrumentSampleRateRef.current = audioContext.sampleRate;
      instrumentBufferRef.current = new Float32Array(0);
      instrumentLastEvalRef.current = 0;
      instrumentInfoRef.current = null;

      // ç¡®ä¿éŸ³é¢‘ä¸Šä¸‹æ–‡è¿è¡Œ
      try {
        if (audioContext.state !== 'running') {
          await audioContext.resume();
          console.log('AudioContext å·²æ¢å¤:', audioContext.state);
        }
      } catch (resumeErr) {
        console.warn('AudioContext æ¢å¤å¤±è´¥:', resumeErr);
      }

      // å¯ç”¨éŸ³è½¨
      try {
        stream.getAudioTracks().forEach(track => {
          if (!track.enabled) track.enabled = true;
        });
      } catch (e) {
        console.warn('æ— æ³•è®¾ç½®éŸ³è½¨å¯ç”¨çŠ¶æ€:', e);
      }

      // é…ç½®åˆ†æå™¨
      analyser.fftSize = 2048;
      analyser.smoothingTimeConstant = 0.5;

      // è¿æ¥
      microphone.connect(analyser);

      // ä¿å­˜å¼•ç”¨
      audioContextRef.current = audioContext;
      analyserRef.current = analyser;
      microphoneRef.current = microphone;
      streamRef.current = stream;

      // åˆå§‹åŒ–å¢å¼ºç‰¹å¾èšåˆå™¨
      try {
        enhancedFeatureAggregatorRef.current = new EnhancedFeatureAggregator();
        await enhancedFeatureAggregatorRef.current.initialize();
        console.log('å¢å¼ºç‰¹å¾èšåˆå™¨åˆå§‹åŒ–å®Œæˆ');
      } catch (e) {
        console.warn('å¢å¼ºç‰¹å¾èšåˆå™¨åˆå§‹åŒ–å¤±è´¥:', e);
      }

      // åˆå§‹åŒ– Meyda ç‰¹å¾æå–
      try {
        if (Meyda && (Meyda as any).isBrowser) {
          meydaAnalyzerRef.current = Meyda.createMeydaAnalyzer({
            audioContext,
            source: microphone,
            bufferSize: 1024,
            featureExtractors: [
              'rms',
              'spectralCentroid',
              'zcr',
              'mfcc',
              'spectralFlatness',
              'spectralFlux',
              'chroma',
              'spectralBandwidth',
              'spectralRolloff',
              'spectralContrast',
              'spectralSpread',
              'spectralSkewness',
              'spectralKurtosis',
              'loudness',
              'perceptualSpread',
              'perceptualSharpness',
            ],
            callback: (f: any) => {
              try {
                const hints = deriveAudioHints(f);
                const instrumentInfo = instrumentInfoRef.current;
                const loudnessValue =
                  typeof f.loudness === 'number'
                    ? f.loudness
                    : typeof f?.loudness?.total === 'number'
                      ? f.loudness.total
                      : undefined;
                const probabilities = instrumentInfo?.probabilities;
                const voiceModel = clampOptional(probabilities?.voice);
                const drumsModel = clampOptional(probabilities?.drums);
                const bassModel = clampOptional(probabilities?.bass);
                const synthModel = clampOptional(probabilities?.synth);
                const percussiveModel = clamp01(
                  drumsModel * 0.7 + bassModel * 0.2 + synthModel * 0.1
                );
                const heuristicVoice = hints.voiceProb ?? 0;
                const heuristicPercussive = hints.percussiveRatio ?? 0;
                const heuristicHarmonic = hints.harmonicRatio ?? undefined;

                const blendedVoice =
                  voiceModel > 0
                    ? clamp01(heuristicVoice * 0.4 + voiceModel * 0.6)
                    : heuristicVoice;
                voiceLevelRef.current =
                  voiceLevelRef.current * 0.7 + blendedVoice * 0.3;
                const finalVoiceProb = clamp01(voiceLevelRef.current);

                const blendedPercussive = clamp01(
                  heuristicPercussive * 0.5 + percussiveModel * 0.5
                );
                percussiveLevelRef.current =
                  percussiveLevelRef.current * 0.7 + blendedPercussive * 0.3;
                const finalPercussive = clamp01(percussiveLevelRef.current);

                const harmonicModel = clamp01(1 - finalPercussive + finalVoiceProb * 0.2);
                const blendedHarmonic =
                  heuristicHarmonic !== undefined
                    ? clamp01(heuristicHarmonic * 0.5 + harmonicModel * 0.5)
                    : harmonicModel;
                harmonicLevelRef.current =
                  harmonicLevelRef.current * 0.6 + blendedHarmonic * 0.4;
                const finalHarmonic = clamp01(harmonicLevelRef.current);

                const snapshot: AudioFeatureSnapshot = {
                  rms: pickNumber(f.rms),
                  spectralCentroid: pickNumber(f.spectralCentroid),
                  zcr: pickNumber(f.zcr),
                  mfcc: toFixedArray(f.mfcc, 13),
                  spectralFlatness: pickNumber(f.spectralFlatness),
                  spectralFlux: pickNumber(f.spectralFlux),
                  chroma: toFixedArray(f.chroma, 12),
                  spectralBandwidth: pickNumber(f.spectralBandwidth),
                  spectralRolloff: pickNumber(f.spectralRolloff),
                  spectralContrast: toFixedArray(f.spectralContrast, 6),
                  spectralSpread: pickNumber(f.spectralSpread),
                  spectralSkewness: pickNumber(f.spectralSkewness),
                  spectralKurtosis: pickNumber(f.spectralKurtosis),
                  loudness: pickNumber(loudnessValue),
                  perceptualSpread: pickNumber(f.perceptualSpread),
                  perceptualSharpness: pickNumber(f.perceptualSharpness),
                  voiceProb: finalVoiceProb,
                  percussiveRatio: finalPercussive,
                  harmonicRatio: finalHarmonic,
                  dominantInstrument: instrumentInfo?.label,
                  instrumentProbabilities: instrumentInfo?.probabilities,
                  instrumentConfidence: instrumentInfo?.probability,
                };

                // æ·»åŠ å¢å¼ºç‰¹å¾æå–
                if (enhancedFeatureAggregatorRef.current && audioContextRef.current) {
                  try {
                    // è·å–å½“å‰éŸ³é¢‘ç¼“å†²åŒº
                    const audioBuffer = new Float32Array(analyser.fftSize);
                    analyser.getFloatTimeDomainData(audioBuffer);
                    const sampleRate = audioContextRef.current.sampleRate;
                    
                    // åˆ›å»ºå¢å¼ºç‰¹å¾å¸§
                    const enhancedFrame = {
                      ...snapshot,
                      audioBuffer,
                      sampleRate,
                      timestamp: Date.now(),
                      // ç¡®ä¿åŒ…å«å¿…éœ€çš„å­—æ®µ
                      pitch: undefined,
                      tempo: undefined,
                      timbre: undefined,
                      instruments: undefined,
                      enhancedHPSS: undefined
                    };

                    // å¼‚æ­¥å¤„ç†å¢å¼ºç‰¹å¾
                    enhancedFeatureAggregatorRef.current.addEnhancedFrame(enhancedFrame).then(() => {
                      // è·å–å¢å¼ºç‰¹å¾ç»Ÿè®¡
                      const enhancedWindow = enhancedFeatureAggregatorRef.current?.getLatestEnhancedWindow();
                      if (enhancedWindow) {
                        // æ›´æ–°å¿«ç…§ä¸­çš„å¢å¼ºç‰¹å¾
                        snapshot.pitch = enhancedWindow.pitchStats ? {
                          fundamentalFreq: enhancedWindow.pitchStats.avgFundamentalFreq,
                          pitchConfidence: enhancedWindow.pitchStats.pitchConfidence,
                          pitchClass: enhancedWindow.pitchStats.dominantPitch,
                          octave: Math.floor(Math.log2(enhancedWindow.pitchStats.avgFundamentalFreq / 16.35)) - 1,
                          cents: 0, // å¯ä»¥ä»è¯¦ç»†ç»Ÿè®¡ä¸­è·å–
                          harmonicity: 0.8, // å¯ä»¥ä»è¯¦ç»†ç»Ÿè®¡ä¸­è·å–
                          isVoiced: enhancedWindow.pitchStats.pitchConfidence > 0.5
                        } : undefined;

                        snapshot.tempo = enhancedWindow.tempoStats ? {
                          bpm: enhancedWindow.tempoStats.avgBpm,
                          tempoConfidence: enhancedWindow.tempoStats.tempoConfidence,
                          timeSignature: enhancedWindow.tempoStats.dominantTimeSignature.split('/').map(Number) as [number, number],
                          rhythmPattern: enhancedWindow.tempoStats.tempoStability > 0.8 ? 'regular' : 'irregular'
                        } : undefined;

                        snapshot.timbre = enhancedWindow.timbreStats ? {
                          brightness: enhancedWindow.timbreStats.avgBrightness,
                          warmth: enhancedWindow.timbreStats.avgWarmth,
                          roughness: enhancedWindow.timbreStats.avgRoughness,
                          timbreCategory: enhancedWindow.timbreStats.dominantTimbre
                        } : undefined;

                        snapshot.instruments = enhancedWindow.instrumentStats ? {
                          dominantInstrument: enhancedWindow.instrumentStats.dominantInstrument,
                          instrumentCount: enhancedWindow.instrumentStats.instrumentCount,
                          polyphony: enhancedWindow.instrumentStats.polyphony
                        } : undefined;

                        snapshot.enhancedHPSS = enhancedWindow.enhancedHPSSStats ? {
                          musicComplexity: enhancedWindow.enhancedHPSSStats.avgMusicalComplexity,
                          overallStability: enhancedWindow.enhancedHPSSStats.avgMusicalStability,
                          overallRichness: enhancedWindow.enhancedHPSSStats.avgMusicalRichness,
                          dominantComponent: 'mixed' as const // ä»ç»Ÿè®¡ä¸­æ¨æ–­
                        } : undefined;

                        console.log('ğŸµ å¢å¼ºç‰¹å¾æå–å®Œæˆ:', {
                          pitch: snapshot.pitch,
                          tempo: snapshot.tempo,
                          timbre: snapshot.timbre,
                          instruments: snapshot.instruments,
                          enhancedHPSS: snapshot.enhancedHPSS
                        });
                      }
                    }).catch(err => {
                      console.warn('å¢å¼ºç‰¹å¾æå–å¤±è´¥:', err);
                    });
                  } catch (err) {
                    console.warn('å¢å¼ºç‰¹å¾å¤„ç†é”™è¯¯:', err);
                  }
                }

                featureSnapshotRef.current = snapshot;
                setFeatures(snapshot);
              } catch (e) {
                // ignore per-frame errors
              }
            },
          });
          meydaAnalyzerRef.current.start();
        }
      } catch (e) {
        console.warn('Meyda åˆå§‹åŒ–å¤±è´¥:', e);
      }

      setStatus('å¼€å§‹éŸ³é¢‘åˆ†æ...');
      console.log('å¼€å§‹éŸ³é¢‘åˆ†æ...');

      // å…ˆæ ‡è®°ä¸ºè¿è¡Œï¼Œå†å¯åŠ¨å¾ªç¯ï¼Œé¿å…é—­åŒ…æ•è·æ—§çŠ¶æ€
      isRunningRef.current = true;
      setIsStarted(true);
      startAudioAnalysis();

      // å¯åŠ¨å¼¹å¹•ç®¡çº¿ï¼ˆå»¶è¿Ÿå¯åŠ¨ç¡®ä¿åˆå§‹åŒ–å®Œæˆï¼‰
      console.log('ğŸµ å¼¹å¹•ç®¡çº¿å‡†å¤‡çŠ¶æ€:', { isReady: danmuPipeline.isReady });
      setTimeout(() => {
        console.log('ğŸµ å»¶è¿Ÿå¯åŠ¨å¼¹å¹•ç®¡çº¿');
        setTimeout(() => danmuPipeline.start(), 500);
      }, 1000); // å»¶è¿Ÿ1ç§’å¯åŠ¨

      setStatus('éŸ³é¢‘åˆ†æå·²å¯åŠ¨');
      setError(null);
      console.log('å¯åŠ¨æˆåŠŸ');
    } catch (err) {
      console.error('å¯åŠ¨å¤±è´¥:', err);
      setError(err instanceof Error ? err.message : 'å¯åŠ¨å¤±è´¥');
      setStatus('å¯åŠ¨å¤±è´¥');
    }
  };

  // ç§»åŠ¨ç«¯æƒé™å¤„ç†
  const handleMobilePermissionGranted = async (
    stream: MediaStream,
    audioContext: AudioContext
  ) => {
    try {
      setStatus('åˆå§‹åŒ–ç§»åŠ¨ç«¯éŸ³é¢‘åˆ†æ...');
      console.log('åˆå§‹åŒ–ç§»åŠ¨ç«¯éŸ³é¢‘åˆ†æ...');

      const analyser = audioContext.createAnalyser();
      const microphone = audioContext.createMediaStreamSource(stream);

      // é…ç½®åˆ†æå™¨
      analyser.fftSize = 2048;
      analyser.smoothingTimeConstant = 0.5;

      // è¿æ¥
      microphone.connect(analyser);

      // ä¿å­˜å¼•ç”¨
      audioContextRef.current = audioContext;
      analyserRef.current = analyser;
      microphoneRef.current = microphone;
      streamRef.current = stream;

      // è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥éŸ³é¢‘æµçŠ¶æ€
      console.log('ğŸ¤ ç§»åŠ¨ç«¯éŸ³é¢‘æµä¿¡æ¯:', {
        audioTracks: stream.getAudioTracks().length,
        trackEnabled: stream.getAudioTracks()[0]?.enabled,
        trackMuted: stream.getAudioTracks()[0]?.muted,
        trackReadyState: stream.getAudioTracks()[0]?.readyState,
        trackLabel: stream.getAudioTracks()[0]?.label,
        audioContextState: audioContext.state,
        analyserConnected: true,
      });

      // ç¡®ä¿éŸ³é¢‘è½¨é“å¯ç”¨ï¼ˆç§»åŠ¨è®¾å¤‡ç‰¹åˆ«é‡è¦ï¼‰
      try {
        stream.getAudioTracks().forEach(track => {
          if (!track.enabled) {
            console.log('å¯ç”¨éŸ³é¢‘è½¨é“...');
            track.enabled = true;
          }
        });
      } catch (e) {
        console.warn('æ— æ³•è®¾ç½®éŸ³é¢‘è½¨é“å¯ç”¨çŠ¶æ€:', e);
      }

      // è®¾ç½®ç§»åŠ¨ç«¯è®¾å¤‡ä¿¡æ¯
      const audioTracks = stream.getAudioTracks();
      if (audioTracks.length > 0) {
        const track = audioTracks[0];
        const deviceLabel = track.label || 'ç§»åŠ¨ç«¯éº¦å…‹é£';
        setDeviceInfo(`ç§»åŠ¨ç«¯è®¾å¤‡: ${deviceLabel}`);
      } else {
        setDeviceInfo('ç§»åŠ¨ç«¯éŸ³é¢‘è®¾å¤‡å·²è¿æ¥');
      }

      // åˆå§‹åŒ– Meyda ç‰¹å¾æå–
      try {
        if (Meyda && (Meyda as any).isBrowser) {
          meydaAnalyzerRef.current = Meyda.createMeydaAnalyzer({
            audioContext,
            source: microphone,
            bufferSize: 1024,
            featureExtractors: [
              'rms',
              'spectralCentroid',
              'zcr',
              'mfcc',
              'spectralFlatness',
              'spectralFlux',
              'chroma',
              'spectralBandwidth',
              'spectralRolloff',
              'spectralContrast',
              'spectralSpread',
              'spectralSkewness',
              'spectralKurtosis',
              'loudness',
              'perceptualSpread',
              'perceptualSharpness',
            ],
            callback: (f: any) => {
              try {
                const hints = deriveAudioHints(f);
                const instrumentInfo = instrumentInfoRef.current;
                setFeatures({
                  rms: typeof f.rms === 'number' ? f.rms : undefined,
                  spectralCentroid:
                    typeof f.spectralCentroid === 'number'
                      ? f.spectralCentroid
                      : undefined,
                  zcr: typeof f.zcr === 'number' ? f.zcr : undefined,
                  mfcc: Array.isArray(f.mfcc) ? f.mfcc : undefined,
                  spectralFlatness:
                    typeof f.spectralFlatness === 'number'
                      ? f.spectralFlatness
                      : undefined,
                  spectralFlux:
                    typeof f.spectralFlux === 'number'
                      ? f.spectralFlux
                      : undefined,
                  chroma: Array.isArray(f.chroma) ? f.chroma : undefined,
                  spectralBandwidth:
                    typeof f.spectralBandwidth === 'number'
                      ? f.spectralBandwidth
                      : undefined,
                  spectralRolloff:
                    typeof f.spectralRolloff === 'number'
                      ? f.spectralRolloff
                      : undefined,
                  spectralContrast: Array.isArray(f.spectralContrast)
                    ? f.spectralContrast
                    : undefined,
                  spectralSpread:
                    typeof f.spectralSpread === 'number'
                      ? f.spectralSpread
                      : undefined,
                  spectralSkewness:
                    typeof f.spectralSkewness === 'number'
                      ? f.spectralSkewness
                      : undefined,
                  spectralKurtosis:
                    typeof f.spectralKurtosis === 'number'
                      ? f.spectralKurtosis
                      : undefined,
                  loudness:
                    typeof f.loudness === 'number' ? f.loudness : undefined,
                  perceptualSpread:
                    typeof f.perceptualSpread === 'number'
                      ? f.perceptualSpread
                      : undefined,
                  perceptualSharpness:
                    typeof f.perceptualSharpness === 'number'
                      ? f.perceptualSharpness
                      : undefined,
                  voiceProb: hints.voiceProb,
                  percussiveRatio: hints.percussiveRatio,
                  harmonicRatio: hints.harmonicRatio,
                  dominantInstrument: instrumentInfo?.label,
                  instrumentProbabilities: instrumentInfo?.probabilities,
                  instrumentConfidence: instrumentInfo?.probability,
                });
              } catch (e) {
                // ignore per-frame errors
              }
            },
          });
          meydaAnalyzerRef.current.start();
        }
      } catch (e) {
        console.warn('Meyda åˆå§‹åŒ–å¤±è´¥:', e);
      }

      setStatus('å¼€å§‹éŸ³é¢‘åˆ†æ...');
      console.log('å¼€å§‹éŸ³é¢‘åˆ†æ...');

      // ç¡®ä¿éŸ³é¢‘ä¸Šä¸‹æ–‡å·²æ¢å¤ (ç§»åŠ¨è®¾å¤‡ç‰¹åˆ«é‡è¦)
      try {
        if (audioContext.state === 'suspended') {
          await audioContext.resume();
          console.log('AudioContext å·²æ¢å¤:', audioContext.state);
        }
      } catch (resumeErr) {
        console.warn('AudioContext æ¢å¤å¤±è´¥:', resumeErr);
      }

      // å…ˆæ ‡è®°ä¸ºè¿è¡Œï¼Œå†å¯åŠ¨å¾ªç¯ï¼Œé¿å…é—­åŒ…æ•è·æ—§çŠ¶æ€
      isRunningRef.current = true;
      setIsStarted(true);
      setShowMobilePermission(false);

      // çŸ­æš‚å»¶è¿Ÿç¡®ä¿éŸ³é¢‘ä¸Šä¸‹æ–‡å®Œå…¨å°±ç»ª (ç§»åŠ¨è®¾å¤‡éœ€è¦)
      setTimeout(() => {
        console.log('å¯åŠ¨éŸ³é¢‘åˆ†æå¾ªç¯...');
        startAudioAnalysis();
      }, 100);

      // å¯åŠ¨å¼¹å¹•ç®¡çº¿
      if (danmuPipeline.isReady) {
        setTimeout(() => danmuPipeline.start(), 500);
      }

      setStatus('ç§»åŠ¨ç«¯éŸ³é¢‘åˆ†æå·²å¯åŠ¨');
      setError(null);
      console.log('ç§»åŠ¨ç«¯å¯åŠ¨æˆåŠŸ');
    } catch (err) {
      console.error('ç§»åŠ¨ç«¯å¯åŠ¨å¤±è´¥:', err);
      setError(err instanceof Error ? err.message : 'ç§»åŠ¨ç«¯å¯åŠ¨å¤±è´¥');
      setStatus('ç§»åŠ¨ç«¯å¯åŠ¨å¤±è´¥');
    }
  };

  // ç§»åŠ¨ç«¯æƒé™é”™è¯¯å¤„ç†
  const handleMobilePermissionError = (error: Error) => {
    console.error('ç§»åŠ¨ç«¯æƒé™é”™è¯¯:', error);
    setError(error.message);
    setShowMobilePermission(false);
    setShowAudioFallback(true);
  };

  // ç§»åŠ¨ç«¯é™çº§é€‰æ‹©å¤„ç†
  const handleMobileFallbackSelected = (type: string) => {
    console.log('ç§»åŠ¨ç«¯é™çº§é€‰æ‹©:', type);
    setShowMobilePermission(false);
    setShowAudioFallback(true);
  };

  // é™çº§éŸ³é¢‘å¯åŠ¨å¤„ç†
  const handleFallbackStarted = (
    audioContext: AudioContext,
    analyser: AnalyserNode
  ) => {
    try {
      console.log('é™çº§éŸ³é¢‘å¯åŠ¨æˆåŠŸ');

      // ä¿å­˜å¼•ç”¨
      audioContextRef.current = audioContext;
      analyserRef.current = analyser;

      // è®¾ç½®é™çº§éŸ³é¢‘è®¾å¤‡ä¿¡æ¯
      setDeviceInfo('é™çº§éŸ³é¢‘æº: æµ‹è¯•éŸ³é¢‘/æ–‡ä»¶ä¸Šä¼ ');

      // æ ‡è®°ä¸ºè¿è¡Œ
      isRunningRef.current = true;
      setIsStarted(true);
      setShowAudioFallback(false);
      setStatus('é™çº§éŸ³é¢‘åˆ†æå·²å¯åŠ¨');
      setError(null);

      // å¯åŠ¨éŸ³é¢‘åˆ†æå¾ªç¯
      startAudioAnalysis();

      // å¯åŠ¨å¼¹å¹•ç®¡çº¿
      if (danmuPipeline.isReady) {
        setTimeout(() => danmuPipeline.start(), 500);
      }
    } catch (err) {
      console.error('é™çº§éŸ³é¢‘å¯åŠ¨å¤±è´¥:', err);
      setError(err instanceof Error ? err.message : 'é™çº§å¯åŠ¨å¤±è´¥');
    }
  };

  // é™çº§éŸ³é¢‘åœæ­¢å¤„ç†
  const handleFallbackStopped = () => {
    console.log('é™çº§éŸ³é¢‘åœæ­¢');
    handleStop();
  };

  const startAudioAnalysis = () => {
    if (!analyserRef.current) return;

    const analyser = analyserRef.current;
    const bufferLength = analyser.fftSize; // ä½¿ç”¨æ—¶åŸŸæ•°æ®é•¿åº¦
    const timeDomainData = new Float32Array(bufferLength);
    // const freqBufferLen = analyser.frequencyBinCount;
    // const freqData = new Uint8Array(freqBufferLen);

    // ä»ä¿ç•™é¢‘è°±ä¸‹é‡‡æ ·å·¥å…·ï¼Œå¦‚åç»­éœ€è¦å¯ç”¨
    // const downsample = (src: Uint8Array, targetBins: number) => {
    //   const bins: number[] = new Array(targetBins).fill(0);
    //   const step = src.length / targetBins;
    //   for (let i = 0; i < targetBins; i++) {
    //     const start = Math.floor(i * step);
    //     const end = Math.floor((i + 1) * step);
    //     let sum = 0;
    //     const s = Math.max(start, 0);
    //     const e = Math.min(end, src.length);
    //     const count = Math.max(1, e - s);
    //     for (let j = s; j < e; j++) sum += src[j];
    //     bins[i] = sum / count / 255;
    //   }
    //   return bins;
    // };

    console.log('å¼€å§‹éŸ³é¢‘åˆ†æå¾ªç¯ï¼Œç¼“å†²åŒºé•¿åº¦:', bufferLength);

    const analyze = () => {
      if (!isRunningRef.current) return;

      // ä¸´æ—¶ç¦ç”¨éŸ³é¢‘åˆ†æï¼Œé¿å…é”™è¯¯
      // try {
      if (testMode) {
        const testLevel = Math.sin(Date.now() * 0.01) * 0.5 + 0.5;
        setAudioLevel(testLevel);
        setPeak(testLevel);
        setSignalOn(testLevel > 0.01);
      } else {
        // ä½¿ç”¨æ—¶åŸŸæ•°æ®è®¡ç®— RMS ä¸å³°å€¼
        analyser.getFloatTimeDomainData(timeDomainData);

        const chunkCopy = new Float32Array(timeDomainData);
        appendInstrumentSamples(chunkCopy);
        runInstrumentDetection();

        let sumSquares = 0;
        let maxAbs = 0;
        let minVal = 1;
        let maxVal = -1;
        let zeroCount = 0;

        for (let i = 0; i < bufferLength; i++) {
          const sample = timeDomainData[i]; // [-1, 1]
          if (sample === 0) zeroCount++;
          sumSquares += sample * sample;
          if (sample > maxVal) maxVal = sample;
          if (sample < minVal) minVal = sample;
          const abs = Math.abs(sample);
          if (abs > maxAbs) maxAbs = abs;
        }

        const rms = Math.sqrt(sumSquares / bufferLength); // [0, 1]
        let normalizedLevel = Math.min(Math.max(rms, 0), 1);

        // è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥æ˜¯å¦æ‰€æœ‰å€¼éƒ½æ˜¯0
        if (zeroCount === bufferLength) {
          console.warn('âš ï¸ éŸ³é¢‘æ•°æ®å…¨ä¸ºé›¶ - å¯èƒ½éŸ³é¢‘æµæœªæ­£ç¡®è¿æ¥');

          // å°è¯•æ¢å¤éŸ³é¢‘ä¸Šä¸‹æ–‡
          if (
            audioContextRef.current &&
            audioContextRef.current.state !== 'running'
          ) {
            console.log('å°è¯•æ¢å¤éŸ³é¢‘ä¸Šä¸‹æ–‡...');
            audioContextRef.current.resume().catch(err => {
              console.warn('æ¢å¤éŸ³é¢‘ä¸Šä¸‹æ–‡å¤±è´¥:', err);
            });
          }
        }

        // æ£€æŸ¥æ˜¯å¦æ˜¯å›ºå®šå€¼ï¼ˆå¦‚91-92%å¯èƒ½è¡¨ç¤ºè¯»å–é—®é¢˜ï¼‰
        if (normalizedLevel > 0.9 && normalizedLevel < 0.93) {
          console.warn(
            'âš ï¸ æ£€æµ‹åˆ°å¯èƒ½çš„å›ºå®šéŸ³é¢‘å€¼:',
            normalizedLevel.toFixed(6)
          );
          console.warn('è¿™é€šå¸¸è¡¨ç¤ºéŸ³é¢‘æ•°æ®è¯»å–æœ‰é—®é¢˜ï¼Œè€Œä¸æ˜¯çœŸå®çš„éº¦å…‹é£è¾“å…¥');

          // è¾“å‡ºå‰10ä¸ªæ ·æœ¬å€¼ç”¨äºè°ƒè¯•
          const sampleValues = Array.from(timeDomainData.slice(0, 10));
          console.log('å‰10ä¸ªéŸ³é¢‘æ ·æœ¬å€¼:', sampleValues);
        }

        // æ¯2ç§’è¾“å‡ºä¸€æ¬¡è°ƒè¯•ä¿¡æ¯
        if (
          !(window as any).__lastAudioDebugLog ||
          Date.now() - (window as any).__lastAudioDebugLog > 2000
        ) {
          (window as any).__lastAudioDebugLog = Date.now();
          console.log('ğŸµ éŸ³é¢‘è°ƒè¯•:', {
            rms: rms.toFixed(6),
            normalizedLevel: normalizedLevel.toFixed(6),
            maxAbs: maxAbs.toFixed(6),
            minVal: minVal.toFixed(6),
            maxVal: maxVal.toFixed(6),
            zeroCount,
            bufferLength,
            audioContextState: audioContextRef.current?.state,
            audioContextRunning: audioContextRef.current?.state === 'running',
          });
        }
        // è‡ªé€‚åº”æ ¡å‡†ï¼šä¼°è®¡å™ªå£°åº•å¹¶æŠ¬å‡åŠ¨æ€èŒƒå›´
        if (autoCalibrate) {
          const prev = noiseFloorRef.current || 0;
          const alpha = normalizedLevel < 0.02 ? 0.01 : 0.002; // å®‰é™æ—¶æ›´å¿«è´´è¿‘åº•å™ª
          const floor = prev * (1 - alpha) + normalizedLevel * alpha;
          noiseFloorRef.current = Math.min(0.05, floor); // åº•å™ªä¸Šé™çº¦ 0.05
          const margin = 0.01; // å®‰å…¨è¾¹è·
          const adj = Math.max(
            0,
            normalizedLevel - (noiseFloorRef.current + margin)
          );
          normalizedLevel = Math.min(1, adj * 6); // æ”¾å¤§å‰©ä½™åŠ¨æ€èŒƒå›´
        }
        setAudioLevel(normalizedLevel);
        setPeak(maxAbs);
        setSignalOn(maxAbs > 0.008);

        // è‡ªåŠ¨è§¦å‘å¼¹å¹•ç®¡çº¿
        if (
          !(window as any).__lastPipelineStatusLog ||
          Date.now() - (window as any).__lastPipelineStatusLog > 5000
        ) {
          (window as any).__lastPipelineStatusLog = Date.now();
          console.log('ğŸµ æ£€æŸ¥å¼¹å¹•ç®¡çº¿çŠ¶æ€:', {
            isActive: danmuPipeline.isActive,
            normalizedLevel,
          });
        }
        const latestFeatures = featureSnapshotRef.current;
        if (danmuPipeline.isActive) {
          if (
            !(window as any).__lastFeatureDebugLog ||
            Date.now() - (window as any).__lastFeatureDebugLog > 3000
          ) {
            (window as any).__lastFeatureDebugLog = Date.now();
            console.log('ğŸµ ç‰¹å¾æ•°æ®çŠ¶æ€:', {
              hasFeatures: !!latestFeatures,
              rms: latestFeatures?.rms,
              spectralCentroid: latestFeatures?.spectralCentroid,
              zcr: latestFeatures?.zcr,
              mfcc: latestFeatures?.mfcc?.length,
              chroma: latestFeatures?.chroma?.length,
              spectralFlatness: latestFeatures?.spectralFlatness,
              spectralFlux: latestFeatures?.spectralFlux,
              hasAllFeatures: !!(
                latestFeatures?.rms !== undefined &&
                latestFeatures?.spectralCentroid !== undefined &&
                latestFeatures?.zcr !== undefined
              ),
              meydaAnalyzerExists: !!meydaAnalyzerRef.current,
              featuresKeys: latestFeatures
                ? Object.keys(latestFeatures)
                : [],
              normalizedLevel,
            });
          }

          if (
            !(window as any).__lastPipelineDebugLog ||
            Date.now() - (window as any).__lastPipelineDebugLog > 3000
          ) {
            (window as any).__lastPipelineDebugLog = Date.now();
            console.log('ğŸµ å¼¹å¹•ç®¡çº¿çŠ¶æ€:', {
              isActive: danmuPipeline.isActive,
              hasFeatures: !!latestFeatures,
              normalizedLevel,
            });
          }

          const pipelineFeatures: AudioFeatureSnapshot = {
            ...(latestFeatures ?? {}),
            rms: latestFeatures?.rms ?? normalizedLevel,
            loudness: latestFeatures?.loudness ?? normalizedLevel * 10,
            dominantInstrument:
              instrumentInfoRef.current?.label ??
              latestFeatures?.dominantInstrument,
            instrumentProbabilities:
              instrumentInfoRef.current?.probabilities ??
              latestFeatures?.instrumentProbabilities,
            instrumentConfidence:
              instrumentInfoRef.current?.probability ??
              latestFeatures?.instrumentConfidence,
          };

          if (
            pipelineFeatures.voiceProb == null ||
            pipelineFeatures.percussiveRatio == null ||
            pipelineFeatures.harmonicRatio == null
          ) {
            const derived = deriveAudioHints(pipelineFeatures);
            pipelineFeatures.voiceProb =
              pipelineFeatures.voiceProb ?? derived.voiceProb;
            pipelineFeatures.percussiveRatio =
              pipelineFeatures.percussiveRatio ?? derived.percussiveRatio;
            pipelineFeatures.harmonicRatio =
              pipelineFeatures.harmonicRatio ?? derived.harmonicRatio;
          }

          danmuPipeline.handleAudioFeatures(normalizedLevel, pipelineFeatures);
        } else {
          if (
            !(window as any).__lastInactiveLog ||
            Date.now() - (window as any).__lastInactiveLog > 5000
          ) {
            (window as any).__lastInactiveLog = Date.now();
            console.log('ğŸµ å¼¹å¹•ç®¡çº¿æœªæ¿€æ´»ï¼Œè·³è¿‡è°ƒç”¨');
          }
        }
        if (normalizedLevel > 0.1) {
          // åªåœ¨æœ‰è¶³å¤ŸéŸ³é¢‘æ—¶æµ‹è¯•
          if (
            !(window as any).__lastSimpleTestLog ||
            Date.now() - (window as any).__lastSimpleTestLog > 5000
          ) {
            (window as any).__lastSimpleTestLog = Date.now();
            console.log('ğŸµ è¶…ç®€å•æµ‹è¯•ï¼šç›´æ¥è§¦å‘å¼¹å¹•ç”Ÿæˆ');
          }
          danmuPipeline.trigger();
        }

        // é¢‘è°±è·å–ä¸ä¸‹é‡‡æ · (å·²ç§»é™¤spectrumé¢„è®¾)
        // if (preset === 'spectrum') {
        //   analyser.getByteFrequencyData(freqData);
        //   setFrequencyBars(downsample(freqData, 64));
        // }

        if (maxAbs < 0.01 && normalizedLevel < 0.005) {
          if (
            !(window as any).__lastQuietLog ||
            Date.now() - (window as any).__lastQuietLog > 2000
          ) {
            (window as any).__lastQuietLog = Date.now();
            console.log(
              'é™éŸ³åŒºé—´è°ƒè¯•: maxAbs=',
              maxAbs.toFixed(4),
              'RMS=',
              normalizedLevel.toFixed(4),
              'min=',
              minVal.toFixed(3),
              'max=',
              maxVal.toFixed(3)
            );
          }
        }
      }

      // ç»§ç»­åˆ†æå¾ªç¯
      animationFrameRef.current = requestAnimationFrame(analyze);
      // } catch (error) {
      //   if (error) {
      //     console.error('éŸ³é¢‘åˆ†æé”™è¯¯:', error);
      //   } else {
      //     console.error('éŸ³é¢‘åˆ†æé”™è¯¯: æœªçŸ¥é”™è¯¯');
      //   }
      //   // ä¸è¦ç»§ç»­å¾ªç¯ï¼Œé¿å…æ— é™é”™è¯¯
      //   // animationFrameRef.current = requestAnimationFrame(analyze);
      // }
    };

    animationFrameRef.current = requestAnimationFrame(analyze);
  };

  const handleStop = () => {
    // åœæ­¢éŸ³é¢‘åˆ†æå¾ªç¯
    isRunningRef.current = false;
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
    }

    // åœæ­¢ Meyda
    try {
      if (meydaAnalyzerRef.current) {
        meydaAnalyzerRef.current.stop();
        meydaAnalyzerRef.current = null;
      }
    } catch (e) {
      console.warn('åœæ­¢ Meyda å¤±è´¥:', e);
    }

    // æ–­å¼€éŸ³é¢‘è¿æ¥
    if (microphoneRef.current) {
      microphoneRef.current.disconnect();
      microphoneRef.current = null;
    }

    // åœæ­¢åª’ä½“æµ
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }

    // å…³é—­éŸ³é¢‘ä¸Šä¸‹æ–‡
    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }

    analyserRef.current = null;

    // åœæ­¢å¼¹å¹•ç®¡çº¿
    danmuPipeline.stop();

    setIsStarted(false);
    setStatus('ç­‰å¾…å¯åŠ¨');
    setAudioLevel(0);
    console.log('å·²åœæ­¢');
  };

  return (
    <main className="min-h-screen bg-black text-white flex flex-col items-center justify-center relative">
      {/* æ ‡é¢˜ */}
      <div className="text-center mb-8 z-10">
        <h1 className="text-4xl font-bold mb-2 bg-gradient-to-r from-blue-400 to-purple-600 bg-clip-text text-transparent">
          SonoScope
        </h1>
        <p className="text-lg text-gray-300">å®æ—¶éŸ³ä¹å¯è§†åŒ–ä¸å¼¹å¹•å¼•æ“</p>
      </div>

      {/* Accretion æ§åˆ¶é¢æ¿ */}
      {isStarted && preset === 'accretion' && (
        <div className="mt-4 z-10 w-[min(92vw,680px)] bg-white/5 border border-white/10 rounded-lg p-4 text-sm">
          <div className="mb-2 text-gray-300 font-semibold">
            Accretion æ§åˆ¶é¢æ¿
          </div>
          <div className="grid md:grid-cols-2 gap-4">
            <div>
              <label className="block text-xs text-gray-400 mb-1">
                çµæ•åº¦ä¸‹é™ sensMin ({accretionCtrl.sensMin.toFixed(2)})
              </label>
              <input
                type="range"
                min={0.85}
                max={1.1}
                step={0.01}
                value={accretionCtrl.sensMin}
                onChange={e =>
                  setAccretionCtrl(v => ({
                    ...v,
                    sensMin: Math.min(
                      parseFloat(e.target.value),
                      v.sensMax - 0.01
                    ),
                  }))
                }
                className="w-full"
              />
            </div>
            <div>
              <label className="block text-xs text-gray-400 mb-1">
                çµæ•åº¦ä¸Šé™ sensMax ({accretionCtrl.sensMax.toFixed(2)})
              </label>
              <input
                type="range"
                min={1.0}
                max={1.5}
                step={0.01}
                value={accretionCtrl.sensMax}
                onChange={e =>
                  setAccretionCtrl(v => ({
                    ...v,
                    sensMax: Math.max(
                      parseFloat(e.target.value),
                      v.sensMin + 0.01
                    ),
                  }))
                }
                className="w-full"
              />
            </div>
            <div>
              <label className="block text-xs text-gray-400 mb-1">
                å¢ç›Šæ¯”ä¾‹ gainScale ({accretionCtrl.gainScale.toFixed(2)})
              </label>
              <input
                type="range"
                min={0.6}
                max={1.8}
                step={0.02}
                value={accretionCtrl.gainScale}
                onChange={e =>
                  setAccretionCtrl(v => ({
                    ...v,
                    gainScale: parseFloat(e.target.value),
                  }))
                }
                className="w-full"
              />
            </div>
            <div>
              <label className="block text-xs text-gray-400 mb-1">
                æ€»ä½“äº®åº¦ overallBoost (
                {(accretionCtrl as any).overallBoost?.toFixed(2) ?? '1.00'})
              </label>
              <input
                type="range"
                min={0.7}
                max={1.6}
                step={0.02}
                value={(accretionCtrl as any).overallBoost ?? 1.0}
                onChange={e =>
                  setAccretionCtrl(v => ({
                    ...v,
                    overallBoost: parseFloat(e.target.value),
                  }))
                }
                className="w-full"
              />
            </div>
            <div>
              <label className="block text-xs text-gray-400 mb-1">
                é—ªçƒå¼ºåº¦ flickerStrength (
                {accretionCtrl.flickerStrength.toFixed(2)})
              </label>
              <input
                type="range"
                min={0.0}
                max={0.35}
                step={0.01}
                value={accretionCtrl.flickerStrength}
                onChange={e =>
                  setAccretionCtrl(v => ({
                    ...v,
                    flickerStrength: parseFloat(e.target.value),
                  }))
                }
                className="w-full"
              />
            </div>
            <div className="md:col-span-2">
              <label className="block text-xs text-gray-400 mb-1">
                é—ªçƒé¢‘ç‡ flickerFreq ({accretionCtrl.flickerFreq.toFixed(0)})
              </label>
              <input
                type="range"
                min={6}
                max={32}
                step={1}
                value={accretionCtrl.flickerFreq}
                onChange={e =>
                  setAccretionCtrl(v => ({
                    ...v,
                    flickerFreq: parseFloat(e.target.value),
                  }))
                }
                className="w-full"
              />
            </div>
          </div>
        </div>
      )}

      {/* Mosaic æ§åˆ¶é¢æ¿ */}
      {isStarted && preset === 'mosaic' && (
        <div className="mt-4 z-10 w-[min(92vw,680px)] bg-white/5 border border-white/10 rounded-lg p-4 text-sm">
          <div className="mb-2 text-gray-300 font-semibold">
            Mosaic æ§åˆ¶é¢æ¿
          </div>
          <div className="grid md:grid-cols-2 gap-4">
            <div>
              <label className="block text-xs text-gray-400 mb-1">
                ç»†èƒå¤§å° cellSize ({mosaicCtrl.cellSize})
              </label>
              <input
                type="range"
                min={10}
                max={50}
                step={2}
                value={mosaicCtrl.cellSize}
                onChange={e =>
                  setMosaicCtrl(v => ({
                    ...v,
                    cellSize: parseInt(e.target.value),
                  }))
                }
                className="w-full"
              />
            </div>
            <div>
              <label className="block text-xs text-gray-400 mb-1">
                æœ€å¤§å¹´é¾„ maxAge ({mosaicCtrl.maxAge})
              </label>
              <input
                type="range"
                min={40}
                max={120}
                step={5}
                value={mosaicCtrl.maxAge}
                onChange={e =>
                  setMosaicCtrl(v => ({
                    ...v,
                    maxAge: parseInt(e.target.value),
                  }))
                }
                className="w-full"
              />
            </div>
            <div>
              <label className="block text-xs text-gray-400 mb-1">
                ç”Ÿé•¿ç‡ growthRate ({mosaicCtrl.growthRate.toFixed(3)})
              </label>
              <input
                type="range"
                min={0.01}
                max={0.1}
                step={0.005}
                value={mosaicCtrl.growthRate}
                onChange={e =>
                  setMosaicCtrl(v => ({
                    ...v,
                    growthRate: parseFloat(e.target.value),
                  }))
                }
                className="w-full"
              />
            </div>
            <div>
              <label className="block text-xs text-gray-400 mb-1">
                ç”Ÿæˆç‡ spawnRate ({mosaicCtrl.spawnRate.toFixed(3)})
              </label>
              <input
                type="range"
                min={0.005}
                max={0.05}
                step={0.002}
                value={mosaicCtrl.spawnRate}
                onChange={e =>
                  setMosaicCtrl(v => ({
                    ...v,
                    spawnRate: parseFloat(e.target.value),
                  }))
                }
                className="w-full"
              />
            </div>
            <div>
              <label className="block text-xs text-gray-400 mb-1">
                é¢œè‰²æµåŠ¨é€Ÿåº¦ colorFlowSpeed (
                {mosaicCtrl.colorFlowSpeed.toFixed(3)})
              </label>
              <input
                type="range"
                min={0.005}
                max={0.05}
                step={0.002}
                value={mosaicCtrl.colorFlowSpeed}
                onChange={e =>
                  setMosaicCtrl(v => ({
                    ...v,
                    colorFlowSpeed: parseFloat(e.target.value),
                  }))
                }
                className="w-full"
              />
            </div>
            <div>
              <label className="block text-xs text-gray-400 mb-1">
                é€æ˜åº¦ alpha ({mosaicCtrl.alpha.toFixed(2)})
              </label>
              <input
                type="range"
                min={0.3}
                max={1.0}
                step={0.05}
                value={mosaicCtrl.alpha}
                onChange={e =>
                  setMosaicCtrl(v => ({
                    ...v,
                    alpha: parseFloat(e.target.value),
                  }))
                }
                className="w-full"
              />
            </div>
            <div className="md:col-span-2">
              <label className="block text-xs text-gray-400 mb-1">
                é¢œè‰²æ–¹æ¡ˆ colorScheme ({mosaicCtrl.colorScheme})
              </label>
              <select
                value={mosaicCtrl.colorScheme}
                onChange={e =>
                  setMosaicCtrl(v => ({
                    ...v,
                    colorScheme: parseInt(e.target.value),
                  }))
                }
                className="w-full px-2 py-1 bg-gray-700 text-white rounded text-xs"
              >
                <option value={0}>é»‘ç™½</option>
                <option value={1}>ç²‰è‰²å¤©ç«ºè‘µ</option>
                <option value={2}>è“è‰²èŠ±æœµ</option>
                <option value={3}>æ—¥è½</option>
                <option value={4}>ç´«è‰²èŠ±æœµ</option>
                <option value={5}>è«å¥ˆ</option>
                <option value={6}>åº·å®šæ–¯åŸº</option>
                <option value={7}>å¤æ—¥</option>
                <option value={8}>æ¨±èŠ±</option>
                <option value={9}>æ¿€æƒ…</option>
                <option value={10}>ç»£çƒèŠ±</option>
                <option value={11}>éƒé‡‘é¦™</option>
                <option value={12}>æµ·æ´‹</option>
                <option value={13}>æ˜äº®</option>
                <option value={14}>æ£®æ—</option>
                <option value={15}>å½©è™¹</option>
                <option value={16}>éœ“è™¹èµ›åš</option>
                <option value={17}>æå…‰</option>
                <option value={18}>ç«ç„°</option>
                <option value={19}>å†°é›ª</option>
                <option value={20}>ç§‹æ—¥</option>
                <option value={21}>æ˜¥æ—¥</option>
                <option value={22}>å®‡å®™</option>
                <option value={23}>æç®€</option>
              </select>
            </div>
          </div>
        </div>
      )}

      {/* é”™è¯¯æ˜¾ç¤º */}
      {error && (
        <div className="mb-4 z-10 p-4 bg-red-900/50 border border-red-500 rounded-lg">
          <p className="text-red-300 text-sm">{error}</p>
        </div>
      )}

      {/* çŠ¶æ€æ˜¾ç¤º */}
      <div className="mb-6 z-10">
        {isStarted ? (
          <div className="flex items-center space-x-2 text-green-400">
            <div className="w-3 h-3 bg-green-400 rounded-full animate-pulse"></div>
            <span>{status}</span>
          </div>
        ) : (
          <div className="flex items-center space-x-2 text-gray-400">
            <div className="w-3 h-3 bg-gray-400 rounded-full"></div>
            <span>{status}</span>
          </div>
        )}

        {/* å¼¹å¹•ç®¡çº¿çŠ¶æ€ */}
        {isStarted && (
          <div className="mt-2 text-xs text-gray-300">
            <div className="flex flex-wrap gap-2">
              <span>å¼¹å¹•: {danmuPipeline.isActive ? 'æ´»è·ƒ' : 'åœæ­¢'}</span>
              {danmuPipeline.currentStyle && (
                <span className="text-blue-400">
                  é£æ ¼: {danmuPipeline.currentStyle}
                </span>
              )}
              <span>æ•°é‡: {danmuPipeline.danmuCount}</span>
              {danmuPipeline.pendingRequests > 0 && (
                <span className="text-yellow-400">
                  ç”Ÿæˆä¸­: {danmuPipeline.pendingRequests}
                </span>
              )}
            </div>
            <div className="mt-1 text-xs text-gray-400">
              æ”¯æŒé£æ ¼: EDM, Techno, Trance, Dubstep, Ambient, Rock, Pop, Jazz,
              Classical, Hip-Hop, Metal
            </div>
          </div>
        )}
      </div>

      {/* éº¦å…‹é£çŠ¶æ€é¢æ¿ */}
      {isStarted && (
        <div className="mb-4 z-10 px-4 py-3 bg-white/5 border border-white/10 rounded-lg text-sm text-gray-200">
          <div className="flex items-center gap-3 flex-wrap">
            <div className="flex items-center gap-2">
              <span
                className={`inline-block w-2.5 h-2.5 rounded-full ${signalOn ? 'bg-green-400' : 'bg-gray-500'}`}
              ></span>
              <span>{signalOn ? 'æœ‰ä¿¡å·' : 'æ— ä¿¡å·'}</span>
            </div>
            <div className="flex items-center gap-2">
              <span className="text-gray-400">å³°å€¼:</span>
              <span>{Math.round(peak * 100)}%</span>
            </div>
            <div className="truncate max-w-[50vw] text-gray-400">
              {deviceInfo || 'è®¾å¤‡ä¿¡æ¯è·å–ä¸­...'}
            </div>
            <div className="flex items-center gap-2 text-gray-400">
              <span>è½¨é“:</span>
              <span>
                {streamRef.current &&
                streamRef.current.getAudioTracks().length > 0
                  ? streamRef.current.getAudioTracks()[0].muted
                    ? 'å·²é™éŸ³'
                    : 'æ´»åŠ¨'
                  : 'æœªè¿æ¥'}
              </span>
            </div>
          </div>
        </div>
      )}

      {/* éŸ³é¢‘çº§åˆ«æŒ‡ç¤ºå™¨ */}
      {isStarted && (
        <div className="mb-6 z-10 w-64">
          <div className="flex items-center justify-between mb-2">
            <span className="text-sm text-gray-400">éŸ³é¢‘çº§åˆ«</span>
            <span className="text-sm text-gray-400">
              {Math.round(audioLevel * 100)}%
            </span>
          </div>
          <div className="w-full bg-gray-700 rounded-full h-2">
            <div
              className="bg-gradient-to-r from-green-500 to-red-500 h-2 rounded-full transition-all duration-100"
              style={{ width: `${audioLevel * 100}%` }}
            ></div>
          </div>
        </div>
      )}

      {/* æ§åˆ¶æŒ‰é’® */}
      <div className="flex gap-4 z-10">
        {/* é¢„è®¾é€‰æ‹© */}
        <select
          value={preset}
          onChange={e =>
            setPreset(
              e.target.value as 'pulse' | 'accretion' | 'spiral' | 'mosaic'
            )
          }
          className="px-3 py-3 bg-gray-700 text-white rounded-lg focus:outline-none focus:ring-2 focus:ring-blue-500"
          aria-label="å¯è§†åŒ–é¢„è®¾"
        >
          <option value="pulse">è„‰å†²åœ†ç¯</option>
          <option value="accretion">Accretion</option>
          <option value="spiral">Spiral</option>
          <option value="mosaic">Mosaic</option>
        </select>

        {/* ååº”å¼ºåº¦ */}
        <div className="flex items-center gap-2 text-gray-300">
          <span className="text-xs">ååº”å¼ºåº¦</span>
          <input
            type="range"
            min={0.5}
            max={3}
            step={0.1}
            value={sensitivity}
            onChange={e => setSensitivity(Number(e.target.value))}
            className="w-32"
            aria-label="ååº”å¼ºåº¦"
          />
          <span className="text-xs w-8 text-right">
            {sensitivity.toFixed(1)}x
          </span>
        </div>

        {/* åŸå§‹éº¦å…‹é£ */}
        <label className="flex items-center gap-2 text-xs text-gray-300">
          <input
            type="checkbox"
            checked={rawMic}
            onChange={e => setRawMic(e.target.checked)}
          />
          åŸå§‹éº¦å…‹é£(å…³é™å™ª/è‡ªåŠ¨å¢ç›Š)
        </label>

        {/* è‡ªé€‚åº”æ ¡å‡† */}
        <label className="flex items-center gap-2 text-xs text-gray-300">
          <input
            type="checkbox"
            checked={autoCalibrate}
            onChange={e => setAutoCalibrate(e.target.checked)}
          />
          è‡ªé€‚åº”æ ¡å‡†
        </label>
        <button
          onClick={handleStart}
          disabled={isStarted}
          className="px-8 py-4 bg-green-600 text-white rounded-lg hover:bg-green-700 focus:outline-none focus:ring-2 focus:ring-green-500 disabled:opacity-50 disabled:cursor-not-allowed transition-all"
        >
          å¼€å§‹
        </button>
        <button
          onClick={handleStop}
          disabled={!isStarted}
          className="px-8 py-4 bg-red-600 text-white rounded-lg hover:bg-red-700 focus:outline-none focus:ring-2 focus:ring-red-500 disabled:opacity-50 disabled:cursor-not-allowed transition-all"
        >
          åœæ­¢
        </button>
        <button
          onClick={() => danmuPipeline.trigger()}
          disabled={!isStarted || !danmuPipeline.isActive}
          className={`px-6 py-4 rounded-lg focus:outline-none focus:ring-2 transition-all ${
            isStarted && danmuPipeline.isActive
              ? 'bg-blue-600 text-white hover:bg-blue-700 focus:ring-blue-500'
              : 'bg-gray-600 text-white opacity-50 cursor-not-allowed'
          }`}
        >
          æ‰‹åŠ¨å¼¹å¹•
        </button>
        <button
          onClick={() => setTestMode(!testMode)}
          className={`px-6 py-4 rounded-lg focus:outline-none focus:ring-2 transition-all ${
            testMode
              ? 'bg-yellow-600 text-white hover:bg-yellow-700 focus:ring-yellow-500'
              : 'bg-gray-600 text-white hover:bg-gray-700 focus:ring-gray-500'
          }`}
        >
          {testMode ? 'é€€å‡ºæµ‹è¯•' : 'æµ‹è¯•æ¨¡å¼'}
        </button>
        <button
          onClick={() => setShowAudioFallback(true)}
          disabled={isStarted}
          className={`px-6 py-4 rounded-lg focus:outline-none focus:ring-2 transition-all ${
            isStarted
              ? 'bg-gray-600 text-white opacity-50 cursor-not-allowed'
              : 'bg-purple-600 text-white hover:bg-purple-700 focus:ring-purple-500'
          }`}
        >
          éŸ³é¢‘é€‰é¡¹
        </button>
      </div>

      {/* è®¾å¤‡ä¿¡æ¯ */}
      {deviceInfo && (
        <div className="mt-4 max-w-lg text-center text-gray-400 z-10">
          <p className="text-xs">{deviceInfo}</p>
        </div>
      )}

      {/* ä½¿ç”¨è¯´æ˜ */}
      <div className="mt-8 max-w-lg text-center text-gray-400 z-10">
        <p className="text-sm">
          ç‚¹å‡»&ldquo;å¼€å§‹&rdquo;æŒ‰é’®æˆæƒéº¦å…‹é£è®¿é—®ï¼Œç„¶åæ’­æ”¾éŸ³ä¹æˆ–å‘å‡ºå£°éŸ³æ¥ä½“éªŒå®æ—¶å¯è§†åŒ–æ•ˆæœ
        </p>
        {!isStarted && (
          <p className="text-xs mt-2 text-yellow-400">
            ğŸ’¡ å¦‚æœæ²¡æœ‰éº¦å…‹é£ï¼Œå¯ä»¥è¿æ¥è€³æœºæˆ–ä½¿ç”¨å†…ç½®éº¦å…‹é£
          </p>
        )}
        {isStarted && (
          <div className="mt-4 text-xs text-gray-500">
            <p>ğŸ”§ è°ƒè¯•ä¿¡æ¯ï¼š</p>
            <p>â€¢ æµ‹è¯•æ¨¡å¼: {testMode ? 'å¼€å¯' : 'å…³é—­'}</p>
            <p>â€¢ éŸ³é¢‘ä¸Šä¸‹æ–‡: {audioContextRef.current ? 'å·²åˆ›å»º' : 'æœªåˆ›å»º'}</p>
            <p>â€¢ åˆ†æå™¨: {analyserRef.current ? 'å·²è¿æ¥' : 'æœªè¿æ¥'}</p>
            <p>â€¢ éº¦å…‹é£: {microphoneRef.current ? 'å·²è¿æ¥' : 'æœªè¿æ¥'}</p>
          </div>
        )}
      </div>

      {/* ç§»åŠ¨ç«¯æƒé™å¼¹çª— */}
      {showMobilePermission && (
        <div className="fixed inset-0 bg-black/80 backdrop-blur-sm flex items-center justify-center z-50 p-4">
          <MobileAudioPermission
            onPermissionGranted={handleMobilePermissionGranted}
            onFallbackSelected={handleMobileFallbackSelected}
            onError={handleMobilePermissionError}
          />
        </div>
      )}

      {/* é™çº§éŸ³é¢‘é€‰é¡¹å¼¹çª— */}
      {showAudioFallback && (
        <div className="fixed inset-0 bg-black/80 backdrop-blur-sm flex items-center justify-center z-50 p-4">
          <div className="w-full max-w-lg max-h-[80vh] overflow-y-auto">
            <AudioFallback
              onFallbackStarted={handleFallbackStarted}
              onFallbackStopped={handleFallbackStopped}
            />
          </div>
        </div>
      )}

      {/* ç”»å¸ƒå®¹å™¨ */}
      <div
        id="visual-canvas"
        className="absolute inset-0 pointer-events-none"
      />
      <div
        id="danmu-container"
        className="absolute inset-0 pointer-events-none"
      />
      <Visualizer
        key={preset}
        audioLevel={
          testMode ? Math.sin(Date.now() * 0.01) * 0.5 + 0.5 : audioLevel
        }
        running={isStarted}
        preset={preset}
        features={features}
        sensitivity={sensitivity}
        accretionControls={accretionCtrl}
        mosaicControls={mosaicCtrl}
      />
    </main>
  );
}
