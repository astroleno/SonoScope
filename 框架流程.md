# 框架流程

## 1. 音频采集与输入
- 浏览器通过 `navigator.mediaDevices.getUserMedia` 获取麦克风流。
- `AudioContext` + `AnalyserNode` 提供实时时域数据，`Meyda` 在 AudioWorklet/主线程上提取 RMS、谱心、MFCC、Chroma 等特征。
- 额外的启发式逻辑输出 `voiceProb`、`percussiveRatio`、`harmonicRatio` 以及最近的主导乐器标签。

## 2. 特征聚合与状态管理
- `FeatureAggregator` 按 2~4 秒窗口滑动聚合帧，计算均值/方差/峰值、动态范围、节拍强度等统计量。
- 新增 `instrumentHistogram`、`instrumentConfidence`，用于描述窗口内乐器占比。
- `checkStability` 输出稳定性指标与置信度，供调度器参考。

## 3. 弹幕调度器
- `DanmuScheduler` 基于 RMS drive 值推导下一次触发间隔和并发上限。
- `DanmuPipelineEnhanced.handleAudioFeatures` 综合 RMS、特征窗口、稳定性，决定是否触发一次评论请求。

## 4. 内容生成链路
1. 触发后调用 `StyleDetector`，依据频谱特征、人声/打击/乐器占比得出风格标签（如 `pop_vocal`、`techno_percussive`）。
2. 调用 `/api/analyze`：
   - 快速规则层 `fastHeuristic` 先给出风格+talking points；
   - 缓存命中直接返回；
   - 否则请求 LLM（智谱/GLM），失败时回退本地模板。
3. LLM/模板返回 `comment` 流，`DanmuPipelineEnhanced` 将评论放入缓冲队列。

## 5. 弹幕缓冲与发射
- `pendingComments` 作为 FIFO 队列；根据最近的 `drive` 值在 3~10 秒区间随机调度发送。
- 每次发送通过 `DanmuEngine.ingestText` 创建 DOM 弹幕元素，按轨道 / 速度进入屏幕。
- 管线停止或过量时会丢弃缓冲评论，避免积压。

## 6. 可视化层
- `Visualizer` 读取音频 level、音频特征、主导乐器等信息，渲染不同 shader/p5 效果（Pulse/Accretion/Spiral）。
- 未来可根据 `dominantInstrument` 自适应切换配色和动画。

## 7. 主要外部依赖
- 音频特征：`meyda`
- 乐器分类（可选）: `@mediapipe/tasks-audio` + `@tensorflow/tfjs`（若网络受限，可在 `ref_model/` 下预置 `yamnet.task`/TFJS 包后改为本地加载）
- 弹幕生成：LLM 接口或本地模板
- 浏览器运行时：Next.js + React + p5

## 8. 数据流概览
```
麦克风流 → 音频特征提取 → FeatureAggregator → DanmuScheduler
  → (达到触发条件) → StyleDetector → /api/analyze (LLM/模板)
  → 评论缓冲队列 → DanmuEngine → 屏幕弹幕
```

## 9. 当前注意事项
- 代理 / 网络受限时需要预先下载 Mediapipe 模型或禁用分类器。
- LLM 返回的多条评论会全部入队，建议结合频率控制与去重策略。
- ESLint 仍有历史 `any` 警告，可逐步治理。
